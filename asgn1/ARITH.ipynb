{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# ARITH - Assignment 1\n",
    "Robert Sato\n",
    "rssato\n",
    "1517254\n",
    "1/13/2021\n",
    "\n",
    "Task:  \n",
    "In Haskell, or the language of your choice, write an interpreter for the ARITH language. Your program should consist of:\n",
    "\n",
    "A data structure for the abstract syntax tree (AST) of ARITH.\n",
    "  \n",
    "- A parser for ARITH.  You may use external libraries when writing the parser. Remember to cite any code that you take from elsewhere.  \n",
    "- Do NOT use regular expressions to parse the string. The ARITH language is simple enough that this would work, but you will still have to worry about precedence. It may be helpful to parse the String into an AST and think about how the AST should be interpreted. You will be asked to write more complicated parsers later in the quarter. Please take the time to learn how to use a real parsing library.  \n",
    "- In HW1, you may assume that there will be exactly one space between numbers and operands, as in the provided test cases. (This assumption will not be true in future homework.)  \n",
    "- An interpreter for this AST.  The interpreter should be in the form of a function called eval, which takes in an AST and returns the result.  \n",
    "- Test cases which show that your AST, parser, and interpreter work.  These test cases should show good code coverage (i.e. test all cases).  \n",
    "- Finally, add a feature to your language, like subtraction or exponentiation.  This addition will involve modifying the AST, parser, and interpreter to support this new feature. You should also write new tests for this feature.  \n",
    "\n",
    "Citations:\n",
    "- https://ruslanspivak.com/lsbasi-part7/ - Found under recommended parsing references"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "To Do:\n",
    "- read in an input string\n",
    "- \"tokenize\" that input string into tokens\n",
    "- create an AST using those tokens\n",
    "- execute the computation using the AST"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTEGER, PLUS, MINUS, MUL, LPAREN, RPAREN, EOF= (\n",
    "    'INTEGER', 'PLUS', 'MINUS', 'MUL', '(', ')', 'EOF'\n",
    ")\n",
    "\n",
    "# token class for representing all the different types of symbols\n",
    "class Token(object):\n",
    "    def __init__(self, type, value):\n",
    "        self.type = type\n",
    "        self.value = value\n",
    "    def __repr__(self):\n",
    "        return 'Token({type}, {value})'.format(\n",
    "            type=self.type,\n",
    "            value=repr(self.value)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"tokenize\" the input string - convert the input text into a list of tokens\n",
    "# accepts numbers, parenthesis, and arithmetic symbols\n",
    "# Given: no decimal numbers and always 1 white space\n",
    "\n",
    "class Lexer(object):\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.pos = 0\n",
    "        self.tokenList = self.tokenize()\n",
    "        self.token_pos = 0\n",
    "    \n",
    "    # return a list of tokens\n",
    "    def tokenize(self):\n",
    "        textAsTokens = []\n",
    "        print(\"tokenize()\\nEquation: \", self.text)\n",
    "        #print(\"Text has len: \", len(self.text))\n",
    "        while self.pos < len(self.text):\n",
    "            curr = self.text[self.pos]\n",
    "            # setup conditionals for all cases\n",
    "            if curr == ' ':\n",
    "                self.pos += 1\n",
    "                continue\n",
    "            elif curr.isdigit():\n",
    "                # keep adding digit chars until whitespace\n",
    "                num = ''\n",
    "                while curr.isdigit(): # keep appending\n",
    "                    num += curr\n",
    "                    self.pos += 1\n",
    "                    if self.pos >= len(self.text):\n",
    "                        break\n",
    "                    curr = self.text[self.pos]\n",
    "                self.pos -= 1\n",
    "                # convert string of digits to int\n",
    "                num = int(num)\n",
    "                textAsTokens.append(Token(INTEGER, num))\n",
    "            elif curr == '+':\n",
    "                textAsTokens.append(Token(PLUS, curr))\n",
    "            elif curr == '*':\n",
    "                textAsTokens.append(Token(MUL, curr))\n",
    "            elif curr == '-':\n",
    "                textAsTokens.append(Token(MINUS, curr))\n",
    "            elif curr == '(':\n",
    "                textAsTokens.append(Token(LPAREN, curr))\n",
    "            elif curr == ')':\n",
    "                textAsTokens.append(Token(RPAREN, curr))\n",
    "            else:\n",
    "                print(\"Error found unknown symbol at '{}'\".format(curr))\n",
    "            \n",
    "            self.pos += 1\n",
    "\n",
    "        return textAsTokens\n",
    "    \n",
    "    # for the parser library\n",
    "    def get_next_token(self):\n",
    "        # can only be called after calling tokenize\n",
    "        #print(\"Token list is size:\", len(self.tokenList))\n",
    "        if self.token_pos < len(self.tokenList):\n",
    "            temp = self.tokenList[self.token_pos]\n",
    "            self.token_pos += 1\n",
    "            return temp\n",
    "        else:\n",
    "            #print(\"No more tokens\")\n",
    "            return Token(EOF, None)\n",
    "\n",
    "    def print(self):\n",
    "        print(\"Text has len: \", len(self.text))\n",
    "        while self.pos < len(self.text):\n",
    "            print(\"{}:{}\".format(self.pos, self.text[self.pos]))\n",
    "            self.pos += 1\n",
    "\n",
    "    def getNextChar(self):\n",
    "        self.pos += 1\n",
    "        return self.text[self.pos-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input text: 123\ntokenize()\nGiven text:  123\nText has len:  3\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'Token' object has no attribute 'name'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-fe19d1e558a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mbreak\u001b[0m \u001b[0;31m# only exec once for now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-116-b65f7f3f13f8>\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         return 'Token({name}, {value})'.format(\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Token' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "# Test block\n",
    "# Read in the input string (inf loop)\n",
    "while True:\n",
    "    text = input('my_parser> ')\n",
    "    print(\"Input text: {}\".format(text))\n",
    "\n",
    "    lexer = Lexer(text)\n",
    "    token = lexer.get_next_token()\n",
    "    while token:\n",
    "        print(token)\n",
    "        token = lexer.get_next_token()\n",
    "    break # only exec once for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tokenize()\nEquation:  1 + 2 * 3\nToken(INTEGER, 1)\nToken(PLUS, '+')\nToken(INTEGER, 2)\nToken(MUL, '*')\nToken(INTEGER, 3)\n\ntokenize()\nEquation:  (1 + 2) * 3\nToken((, '(')\nToken(INTEGER, 1)\nToken(PLUS, '+')\nToken(INTEGER, 2)\nToken(), ')')\nToken(MUL, '*')\nToken(INTEGER, 3)\n\ntokenize()\nEquation:  ((1+ 2)) * 3\nToken((, '(')\nToken((, '(')\nToken(INTEGER, 1)\nToken(PLUS, '+')\nToken(INTEGER, 2)\nToken(), ')')\nToken(), ')')\nToken(MUL, '*')\nToken(INTEGER, 3)\n\ntokenize()\nEquation:  15 * (25 + 10)\nToken(INTEGER, 15)\nToken(MUL, '*')\nToken((, '(')\nToken(INTEGER, 25)\nToken(PLUS, '+')\nToken(INTEGER, 10)\nToken(), ')')\n\n"
     ]
    }
   ],
   "source": [
    "# test cases\n",
    "formula1 = '1 + 2 * 3'\n",
    "formula2 = '(1 + 2) * 3'\n",
    "formula3 = '((1+ 2)) * 3'\n",
    "formula4 = '15 * (25 + 10)'\n",
    "\n",
    "formulas = []\n",
    "formulas.append('1 + 2 * 3')\n",
    "formulas.append('(1 + 2) * 3')\n",
    "formulas.append('((1+ 2)) * 3')\n",
    "formulas.append('15 * (25 + 10)')\n",
    "\n",
    "for formula in formulas:\n",
    "    lexer = Lexer(formula)\n",
    "    token = lexer.get_next_token()\n",
    "    while token.type != EOF:\n",
    "        print(token)\n",
    "        token = lexer.get_next_token()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an AST from the tokens\n",
    "# Parser code from https://ruslanspivak.com/lsbasi-part7/ in references;\n",
    "# changed for my list implementation of tokens\n",
    "class AST(object):\n",
    "    pass\n",
    "\n",
    "class BinOp(AST):\n",
    "    def __init__(self, left, op, right):\n",
    "        self.left = left\n",
    "        self.token = self.op = op\n",
    "        self.right = right\n",
    "\n",
    "class Num(AST):\n",
    "    def __init__(self, token):\n",
    "        self.token = token\n",
    "        self.value = token.value\n",
    "\n",
    "\n",
    "class Parser(object):\n",
    "    def __init__(self, lexer):\n",
    "        self.lexer = lexer\n",
    "        # set current token to the first token taken from the input\n",
    "        self.current_token = self.lexer.get_next_token()\n",
    "\n",
    "    def error(self):\n",
    "        raise Exception('Invalid syntax')\n",
    "\n",
    "    def eat(self, token_type):\n",
    "        # compare the current token type with the passed token\n",
    "        # type and if they match then \"eat\" the current token\n",
    "        # and assign the next token to the self.current_token,\n",
    "        # otherwise raise an exception.\n",
    "        if self.current_token.type == token_type:\n",
    "            self.current_token = self.lexer.get_next_token()\n",
    "        else:\n",
    "            self.error()\n",
    "\n",
    "    def factor(self):\n",
    "        \"\"\"factor : INTEGER | LPAREN expr RPAREN\"\"\"\n",
    "        token = self.current_token\n",
    "        if token.type == INTEGER:\n",
    "            self.eat(INTEGER)\n",
    "            return Num(token)\n",
    "        elif token.type == LPAREN:\n",
    "            self.eat(LPAREN)\n",
    "            node = self.expr()\n",
    "            self.eat(RPAREN)\n",
    "            return node\n",
    "\n",
    "    def term(self):\n",
    "        \"\"\"term : factor ((MUL) factor)*\"\"\"\n",
    "        node = self.factor()\n",
    "\n",
    "        while self.current_token.type in (MUL):\n",
    "            token = self.current_token\n",
    "            if token.type == MUL:\n",
    "                self.eat(MUL)\n",
    "\n",
    "            node = BinOp(left=node, op=token, right=self.factor())\n",
    "\n",
    "        return node\n",
    "\n",
    "    def expr(self):\n",
    "        \"\"\"\n",
    "        expr   : term ((PLUS | MINUS) term)*\n",
    "        term   : factor ((MUL | DIV) factor)*\n",
    "        factor : INTEGER | LPAREN expr RPAREN\n",
    "        \"\"\"\n",
    "        node = self.term()\n",
    "\n",
    "        while self.current_token.type in (PLUS, MINUS):\n",
    "            token = self.current_token\n",
    "            if token.type == PLUS:\n",
    "                self.eat(PLUS)\n",
    "            elif token.type == MINUS:\n",
    "                self.eat(MINUS)\n",
    "\n",
    "            node = BinOp(left=node, op=token, right=self.term())\n",
    "\n",
    "        return node\n",
    "\n",
    "    def parse(self):\n",
    "        return self.expr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printAST(node):\n",
    "    #print(\"type:\", type(node))\n",
    "    if type(node) == BinOp:\n",
    "        printAST(node.left)\n",
    "        printAST(node.right)\n",
    "        print(node.op)\n",
    "    else:\n",
    "        print(node.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tokenize()\nEquation:  1 + 2 * 3\n1\n2\n3\nToken(MUL, '*')\nToken(PLUS, '+')\n\ntokenize()\nEquation:  (1 + 2) * 3\n1\n2\nToken(PLUS, '+')\n3\nToken(MUL, '*')\n\ntokenize()\nEquation:  ((1+ 2)) * 3\n1\n2\nToken(PLUS, '+')\n3\nToken(MUL, '*')\n\ntokenize()\nEquation:  15 * (25 + 10)\n15\n25\n10\nToken(PLUS, '+')\nToken(MUL, '*')\n\ntokenize()\nEquation:  15 * 3 + 2 * 9 + (11 * 2) + (3 + 5 * 2)\n15\n3\nToken(MUL, '*')\n2\n9\nToken(MUL, '*')\nToken(PLUS, '+')\n11\n2\nToken(MUL, '*')\nToken(PLUS, '+')\n3\n5\n2\nToken(MUL, '*')\nToken(PLUS, '+')\nToken(PLUS, '+')\n\n"
     ]
    }
   ],
   "source": [
    "# test cases\n",
    "formulas = []\n",
    "formulas.append('1 + 2 * 3')\n",
    "formulas.append('(1 + 2) * 3')\n",
    "formulas.append('((1+ 2)) * 3')\n",
    "formulas.append('15 * (25 + 10)')\n",
    "formulas.append('15 * 3 + 2 * 9 + (11 * 2) + (3 + 5 * 2)')\n",
    "\n",
    "for formula in formulas:\n",
    "    lexer = Lexer(formula)\n",
    "    parser = Parser(lexer)\n",
    "    node = parser.parse()\n",
    "    printAST(node)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an interpretor that does the computation given the AST\n",
    "class Interpretor(object):\n",
    "    def __init__(self, rootAST):\n",
    "        self.rootAST = rootAST\n",
    "\n",
    "    def interpret(self):\n",
    "        return self.visit(self.rootAST)\n",
    "\n",
    "    def visit(self, node):\n",
    "        # call operation for appropriate BinOp or just Number type\n",
    "        node_type = type(node).__name__\n",
    "        if node_type ==  'BinOp':\n",
    "            if node.op.type == PLUS:\n",
    "                return self.visit(node.left) + self.visit(node.right)\n",
    "            elif node.op.type == MINUS:\n",
    "                return self.visit(node.left) - self.visit(node.right)\n",
    "            elif node.op.type == MUL:\n",
    "                return self.visit(node.left) * self.visit(node.right)\n",
    "        else:\n",
    "            return node.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tokenize()\nEquation:  1 + 2 * 3\n= 7\ntokenize()\nEquation:  (1 + 2) * 3\n= 9\ntokenize()\nEquation:  ((1+ 2)) * 3\n= 9\ntokenize()\nEquation:  15 * (25 + 10)\n= 525\ntokenize()\nEquation:  15 * 3 + 2 * 9 + (11 * 2) + (3 + 5 * 2)\n= 98\n"
     ]
    }
   ],
   "source": [
    "# test cases\n",
    "formulas = []\n",
    "formulas.append('1 + 2 * 3')\n",
    "formulas.append('(1 + 2) * 3')\n",
    "formulas.append('((1+ 2)) * 3')\n",
    "formulas.append('15 * (25 + 10)')\n",
    "formulas.append('15 * 3 + 2 * 9 + (11 * 2) + (3 + 5 * 2)')\n",
    "\n",
    "for formula in formulas:\n",
    "    lexer = Lexer(formula)\n",
    "    parser = Parser(lexer)\n",
    "    node = parser.parse()\n",
    "    interpretor = Interpretor(node)\n",
    "    val = interpretor.interpret()\n",
    "    print(\"=\", val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}